% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SL.xgboost.R
\name{SL.xgboost}
\alias{SL.xgboost}
\alias{predict.SL.xgboost}
\title{SL wrapper for XGBoost}
\usage{
SL.xgboost(
  Y,
  X,
  newX = X,
  family = gaussian(),
  obsWeights = NULL,
  ntrees = 1000,
  max_depth = 4,
  shrinkage = 0.1,
  minobspernode = 10,
  params = list(),
  nthread = 1,
  verbose = 0,
  save_period = NULL,
  ...
)

\method{predict}{SL.xgboost}(object, newdata, ...)
}
\arguments{
\item{Y}{The outcome in the training data set. Must be a numeric vector.}

\item{X}{The predictor variables in the training data set, usually a data.frame.}

\item{newX}{The predictor variables in the validation data set. The
structure should match X.}

\item{family}{Either \code{\link[=gaussian]{gaussian()}} or \code{\link[=binomial]{binomial()}} to
describe the error distribution. Link function information will be ignored.}

\item{obsWeights}{Optional observation weights.}

\item{ntrees}{How many trees to fit. Low numbers may underfit but high
numbers may overfit, depending also on the shrinkage.}

\item{max_depth}{How deep each tree can be. 1 means no interactions, aka tree
stubs.}

\item{shrinkage}{How much to shrink the predictions, in order to reduce
overfitting.}

\item{minobspernode}{Minimum observations allowed per tree node, after which
no more splitting will occur.}

\item{params}{Many other parameters can be customized. See
\url{http://xgboost.readthedocs.io/en/latest/parameter.html}}

\item{nthread}{How many threads (cores) should xgboost use. Generally we want
to keep this to 1 so that XGBoost does not compete with SuperLearner
parallelization.}

\item{verbose}{Verbosity of XGB fitting.}

\item{save_period}{How often (in tree iterations) to save current model to
disk during processing. If NULL does not save model, and if 0 saves model
at the end.}

\item{...}{Any remaining arguments (not supported though).}

\item{object}{an object resulting from the fitting function.}

\item{newdata}{a data frame of predictors for which to compute predictions.}
}
\description{
Supports the Extreme Gradient Boosting package for SuperLearning, which is
a variant of gradient boosted machines (GBM).
}
\details{
The performance of XGBoost, like GBM, is sensitive to the configuration
settings. Therefore it is best to create multiple configurations using
create.SL.xgboost and allow the SuperLearner to choose the best weights based
on cross-validated performance.

If you run into errors please first try installing the latest version of
XGBoost from drat as described here:
\url{http://xgboost.readthedocs.io/en/latest/build.html}
}
\seealso{
\code{\link[=create.SL.xgboost]{create.SL.xgboost()}} to create new xgboost wrappers with different parameters.
}
