% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SL.glmnet.R
\name{SL.glmnet}
\alias{SL.glmnet}
\alias{predict.SL.glmnet}
\title{SL wrapper for elastic net regression, including lasso and ridge}
\usage{
SL.glmnet(
  Y,
  X,
  newX,
  family,
  obsWeights,
  id,
  alpha = 1,
  nfolds = 10,
  nlambda = 100,
  useMin = TRUE,
  loss = "deviance",
  ...
)

\method{predict}{SL.glmnet}(
  object,
  newdata,
  remove_extra_cols = TRUE,
  add_missing_cols = TRUE,
  ...
)
}
\arguments{
\item{Y}{The outcome in the training data set. Must be a numeric vector.}

\item{X}{The predictor variables in the training data set, usually a data.frame.}

\item{newX}{The predictor variables in the validation data set. The
structure should match X.}

\item{family}{Either \code{\link[=gaussian]{gaussian()}} or \code{\link[=binomial]{binomial()}} to
describe the error distribution. Link function information will be ignored.}

\item{obsWeights}{Optional observation weights.}

\item{id}{Optional cluster identification variable.}

\item{alpha}{Elastic net mixing parameter, range [0, 1]. 0 = ridge regression
and 1 = lasso.}

\item{nfolds}{Number of folds for internal cross-validation to optimize lambda.}

\item{nlambda}{Number of lambda values to check, recommended to be 100 or more.}

\item{useMin}{If TRUE use lambda that minimizes risk, otherwise use 1
standard-error rule which chooses a higher penalty with performance within
one standard error of the minimum (see Breiman et al. 1984 on CART for
background).}

\item{loss}{Loss function, can be "deviance", "mse", or "mae". If family =
binomial can also be "auc" or "class" (misclassification error).}

\item{...}{Any additional arguments are passed through to \code{cv.glmnet()}.}

\item{object}{an object resulting from the fitting function.}

\item{newdata}{a data frame of predictors for which to compute predictions.}

\item{remove_extra_cols}{Remove any extra columns in the new data that were
not part of the original model.}

\item{add_missing_cols}{Add any columns from original data that do not exist
in the new data, and set values to 0.}
}
\description{
Penalized regression using elastic net. Alpha = 0 corresponds to ridge
regression and alpha = 1 corresponds to Lasso.

See \code{vignette("glmnet_beta", package = "glmnet")} for a nice tutorial on
glmnet.
}
\examples{

# Load a test dataset.
data(PimaIndiansDiabetes2, package = "mlbench")
data = PimaIndiansDiabetes2

# Omit observations with missing data.
data = na.omit(data)

Y = as.numeric(data$diabetes == "pos")
X = subset(data, select = -diabetes)

set.seed(1, "L'Ecuyer-CMRG")

sl = SuperLearner(Y, X, family = binomial(),
                  SL.library = c("SL.mean", "SL.glm", "SL.glmnet"))
sl

}
\references{
Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of statistical
software, 33(1), 1.

Hoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation
for nonorthogonal problems. Technometrics, 12(1), 55-67.

Tibshirani, R. (1996). Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society. Series B (Methodological), 267-288.

Zou, H., & Hastie, T. (2005). Regularization and variable selection via the
elastic net. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 67(2), 301-320.
}
\seealso{
\code{\link{predict.SL.glmnet}} \code{\link[glmnet]{cv.glmnet}}
\code{\link[glmnet]{glmnet}}
}
